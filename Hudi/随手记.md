HoodieWriteStat 和 HoodieDeltaWriteStat 区别，且每个 HoodieWriteStat 的具体含义？MOR 写数据时都是 HoodieDeltaWriteStat。

在 commit 的过程中会进行合并

org.apache.hudi.client.HoodieFlinkWriteClient#commit

```java
public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata,
                    String commitActionType, Map<String, List<String>> partitionToReplacedFileIds,
                    Option<BiConsumer<HoodieTableMetaClient, HoodieCommitMetadata>> extraPreCommitFunc) {
List<HoodieWriteStat> writeStats = writeStatuses.parallelStream().map(WriteStatus::getStat).collect(Collectors.toList());
// for eager flush, multiple write stat may share one file path.
List<HoodieWriteStat> merged = writeStats.stream()
    .collect(Collectors.groupingBy(writeStat -> writeStat.getPartitionPath() + writeStat.getPath()))
    .values().stream()
    .map(duplicates -> duplicates.stream().reduce(WriteStatMerger::merge).get())
    .collect(Collectors.toList());
return commitStats(instantTime, merged, extraMetadata, commitActionType, partitionToReplacedFileIds, extraPreCommitFunc);
}
```

按分区进行 groupby，然后对每个分区下进行一次 reduce，合并 record 的 stat。

每个 cp 对应一个 instant time（也是该次 delta commit 的 time），所以两次 cp 间隔之间写入的数据都使用同一个 instant time，可以合并。



schedule 的操作：

-   ARCHIVE
-   CLUSTER
-   COMPACT
-   LOG_COMPACT
-   CLEAN



\<data file name>.marker.\<type>

\<data file name>: basefile name，fileId_writeToken_instantTime.parquet

\<type>: 对应 Hoodie 的 Handler，APPEND MERGE CREATE



buck insert -> HoodieRowDataCreateHandle

upsert -> FlinkAppendHandle(HoodieAppendHandle)  merge on read





insert overwrite -> FlinkMergeHandle or FlinkMergeAndRelapceHandle





注意 org.apache.hudi.client.AbstractHoodieWriteClient#scheduleTableServiceInternal 

cluster compaction clean 都走这里，三个都可以异步



问题

rollback、restore、savepoint 区别



markfile 的存在意义？



clean 能否清除 savepoint

